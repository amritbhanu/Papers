- Reinforcement learning - Kind of a feedback loop.
- attributes - distinct, order, addition, multiplication.
- qualitative (nominal(distinctive), ordinal(distinctive, order)), quantitative (interval (distinctive, order, addition), ratio(all 4))
- ordinal attributes are discrete - true. All ratio attributes are continuous-false, example-age

- video 2, min 50
- outlier can be there but not the noise.
- z-score= (x-mean)/std. Assumption - we dont have 2 different classes. Classes are same. It can range from [-2,2]

- video 3, min 8
- Aggregation - OLAP (Online analytical processing) data cubes
- read advantages and disadvantages
- sampling with replacement, add back again. DRAWBACK OF SIR - if the class is very unbalanced. So, we use stratified sampling, ie picking a percent of numbers from each group of class.
- progressive sampling
- different types of feature subset selection, ie which attributes are really important and which are not.
- Dimensionality reduction - PCA, SVD (Singular Value Decomposition)
- PCA - a rule of thumb - pick eigenvalue > 1
  - Do STANDARDISATION BEFORE DOING PCA??
- cov(x,y) - if positive that means 2 features are changing in the same direction if negative then they are changing in opp direction. If 0 then they are independent.
- (Standardization = (X-mean)/sd), normalization(noramlize the nos to range between [0, 1]). 

- See the video lecture 5 for Jaccard similarity for assymetric attributes.
- Simple matching ((M11+M00)/all). Jaccard = (M11/(M10+M01+M11))
- Correlation = Cov(x,y)/(var(x)*var(y)). Correlation will be between [-1,1]
- unimodal, bimodal, trimodal, when we have many modes.
- mean, variance are more susceptible to outliers.

Video 6

- OLAP - dimensions need to be discrete values. So we did a discretization of petal length of iris dataset. X asis = equal width interval, percentile graph will be equal frequency discretization.
- Slicing - specifying range of values for one or more dimensions, dicing - subset of cells by specifying range of attribute values. Roll up - reduce the cells. Drill down - increase the cells.
- Decision tree - We can build multiple tress for the same data. But the performance and complexity matters. Like height of the tree and how short the tree is.
  - Splitting on binary split needs to be taken care of. We dont want (small, large) in one and (medium in another). We can do (small, medium) in one and (large in another).
  - We have different splitting criteria for different types of attributes ie nominal, ordinal and continuous.
  - Entropy, information gain, gini index. More entropy then less confident about the outcome. min - 51
  - info gain need to be more. Gini index needs to be less just like entropy.
  - nodes(attribute) with homogeneous class distribution is a better feature to split with.
  - how to find for regression model? HOW DECISION TREE REGRESSION WORKS?

Video 7

- overfitting when training error is less and testing error is more
- underfitting when training error is more and testing error is less.
- optimistic error and pessimistic error, for post pruning. Watch 46min
- addressing pre pruning or overfitting

Video 8

- Training error= Only on training data. When we optimize (min) training error and more testing error, then we generally overfit.
- Generalizaition error = on whole population. Thats why we do a testing error. ANd we have 2 approaches
- optimistic approach = my training error is the true error. and pessimistic = e(t) + N*0.5
- occams rajor = prefer simple model if generalization errors are same for 2 models.
- min 37. Pessimistic error more than before splitting then we dont prune.
- for optimistic, if after pruning < before then keep else split, just same as pessimistic approach
- prepruning and post pruning
- video 1:01:00 handling missing value. adding weights to the missing value

WATCH VIDEO 9

- Recal Precision, ROC - TPR (y-axis) = Also recall vs FPR (x-axis) = c/(c+d)
- watch 38:40

Video 10:

- hold out method. That is if you are using different models then keep the training (2/3) and testing (1/3) set consistent across those models. Cheap but high variance
- k-fold cross validation at min 21. For min error divide by no of data points not with k
- leave one out cross validation - only leave one data point out for testing set. Expensive but better evaluation measure.
- cant compare MSE across different cross validations.
- stratification, oversampling and undersampling??
- bootstrapping?

Video 11:

- Ensemble learners. Error rate = 0.35, 25 learners what will be the error rate of ensemble learners? Min 5
- bagging - watch min 20
- adaboost - adaptive boosting - in each round the misclassified record will get more probability of getting chosen in the next training data.
  - weight, alpha, m steps.
  - very important

Video 12:

- eager (decision tree) and lazy learners
- instance based learning - rote learner, knn
- knn - try to scale using (x-min)/(max-min). Normalization
- bayesian learners - conditional probability P(X|Y) = P(X and Y) / P (Y)
- Independence based on conditional property.
- Bayes rule => P(Y|X) = [P(X|Y)*P(Y)] / P(X)
- remember the product rule formula, min 42.

Video 13:

- Naive bayes classifier
- only independent variables so true for naive bayes. Other bayes theorm its not so simple.

Video 14:

- Bayesian Network = 2 properties, conditional independence relationships, and compact representation.
- DAG - Start from vertex V and you will never loop back to the same V.
- if P(C) given the parent B = P(C|B)
- in the table if you have a boolean variable with k boolean parents, this table has 2^(k+1) probabilities (but only 2^k need to be stored). Because P(C|B) + P(-C|B) = 1
- markov assumption : bayesian network is independent of all its ancestors given values of parents as CP of parent will include all its ancestors.
- 2^k - 1 are total parameters needed in using full joint probability distribution on the other hand for baesian network we only need 2^k * n (no of nodes at max).
- watch 23 min. How many parameters are needed? For bayesian network you will need at that point 2^(k-1). This one will change when we have different options for each parameters.
- D - separation, how info is carried along the bayesian networks?

Video 15:

- watch 7:30
- serial, divergent (if we have evidence then its blocked and d-separted), converging or its subchild (its not in evidence, then blocked and d-separated and are independent.) connection.

Video 16:

- find the best line that will fit the data. Regression tries to learn the weights of each attribute.
- watch 15:00
- sum of square error (minimise SSE). Take derivative wrt to w and set to 0.
- if the noise is gaussian with mean 0 then the least squares is also the maximum likelihood estimate of w.
- w= sum (yi*xi)/ sum(xi^2), when no bias
- learn various derivative formulas.
- what if the line doesnt pass from the center, just change the line function. That means bias
- w_0=1/n * sum (y-w_1x)
- w_1= sum (x(y-w_0))/ sum(x^2)
- big X but less data points so we mostly overfit the data. 2 types: ridge regression and lasso regression.
- ridge, will minimise the SSE and also choose only those w which are important. L2 euclidean regression. There is a lambda in it to make it to 0.
- lasso does linear L1 regression, L1 is more aggressive to pushing the w to 0 than ridge..
- multivariate regression - as long as coefficients are linear they stay linear regression.
- its not true more complex the model the better accuracy will be it might be overfitting.
- splines, for different regions, we have different regression function.
- separate data based on density, combination of regression and knn. Closer the points more weight we have.
- advantages and disadvantages

Video 17:

- gradient descent is done because sometimes finding the solutions of weight is not possible. So how GD works is finding alpha in which by changing a*w how well error is improving that means error is decreasing.
- how global optimum or local optimum is found by derivative. Slope should be close to [0,-1)

- NEURAL NETWORKS:

   - Synapse (incoming signal) comes and once it reaches an energy threshold, it fires another signal to other neurons.
   - increasing the size of hidden layer makes the network more powerful but introduce the risk of overfitting. Usual are feedforward network.
   - Single layer neural networks are called perceptrons.
   - neutral network died because of less recognizable patters, expensive training time. But now it is in picture because of deep learning.
   - sigmoid funtion for activation. (setting a sigma, more sigma, then more steeper) 1/(1+e^(-sigma*x))
   - weighted feedforward network.
   - Training NN means learning the weights of neurons. Watch video 39:00 on how to update weights and threshold.
   - if there is a linear line that can separate 2 classes in perceptron, thats what we want.
   - xor problem and we need 2 layers to solve.
   - different activation functions are needed.
   - adv = high accuracy, can handle any type of inputs (continuous, discrete), fast classification once trained.
   - disadv = overfitting problem, no explicit knowledge.

Video 18:

- y(wx_i+w_0) -1 >= 0 that means all data points are either above or below the hyperplane. If it is =1 then these points are more ambiguous and they are on the hyperplane. These points which lie on hyperplane are also called support vectors.
- we want to maximise the hyperplane that is d+ + d- = 2/||w||_L2 and also constraints on points that y(wx_i+w_0) -1 >= 0 always true
- watch 44:00
- a>= 0, b anything, g(x)<=0,h(x)=0, primal min x then max a,b, dual oppo
- dual <=primal weak duality.

Video 19:

- a>0 when on hyperplane else a=0 when on either side.
- thats why now we only have to check for Support vector points.
- non separable case we will have upper bound on C.
- c=0, no penalty, then all points are support vectors, else c=infintiy, then only 2 points as SV.

Video 20: 

- Correlation coefficient = Cov(x,y)/(std(x)*std(y))
-  binary bits you cant use euclidean measure. Use hamming distance (even for categorical, nominal, discrete).
-  parition clustering == kmeans for continuous values, kmedoids for categorical or discrete values.
-   voronoi diagram = perpendicular to the line joining the centers.
-   results are quite sensitive to seed selection of initial random clusters. 
-   results sensitive to initial values.
-   adv - quick, and efficient
-   disadv - can perform poorly with overlapping regions. Lack of robustness to outliers, different sizes, different densities, non globular shapes. 
	-   overcoming this problem by having high no of clusters and then combine them.
-   some seeds can result in poor convergence rate or convergence to sub optimal clustering.
-   seed choice selection = select good seeds using a heuristic, try out multiple starting points (probability still though is not on our side), initialize with the results of another method.
-   sample and use hierarchical clustering, postprocessing, bisecting k-means.
-   handling empty clusters = choose the point that contributes most to SSE.
-   preprocessing = normalize the data, remove outliers
-   postprocessing = eliminate small clusters, split loose clusters ie clusters with relatively high SSE, merge clusters that are close and have relatively low SSE.
-   if there is a tie, then we always select left to right and then top to bottom.
-   intracluster SSE should be close to 0 and intercluster SSE should be as high as possible.


Video 21:

- Hierarchical clustering == agglomerative (bottom up), divisive (top down)
- dendogram (a tree based hierarchical taxonomy)
- strengths = no needs of k
- how to computer proximity matrix. It can be based on distance.
- how to merge to clusters (closest pair of clusters) = watch 38:26, single link, complete link, centroid, average.
- dendogram height will mean the merging value between clusters.
- single link= closest will be merged and then only one point closest will be copied to the new one. Watch 43:00
- complete link= closest will be merged and then only one point which the farthest will be copied to the new one. Watch 46:30. Definitely watch 46:30 as examples changed.
- for cluster similarity, we choose the biggest same no. And follows similarly for single and complete link. Max for single, and min for complete.
-  adv - single link can handle non-elliptical shapes, disadv = sensitive to noise and outliers.
-  adv - complete link less sensitive to outliers and noise, disadv = tends to break large clusters, and biased towards globular clusters.

Video 22:

-  average method = Group average of both the clusters.
-  Ward's method 
    -  adv = less sensitive to noise and outliers. Disadv = biased towards globular clusters.
    -  similarity of two clusters is based on increase in squared error when two clusters are merged.
- limitations = once 2 clusters are combined it cant be undone.
- watch 14:15
- divisive hierarchical clustering (MST)
    - once MST is built then find 2 points who are farthest in distance (less similar) to divide.
- DBSCAN = within a specified radius (eps)
    - core point, set of points which are always more than Minpts within eps
    - border point, less than or equal to minpts but in the neighborhood to core point.
    - noise point which are neither core nor border.
    - Adv = not sensitive to noise, can handle different shape and sizes.
    - disadv = varying density, high dimensional data
- determining eps and minpts, kth highest neighbor then find farthest and growing graph. exponential increase will be the cut point. See video 45:00
- internal measures, SSE = dropping curve, and cut where it becomes constant.
- external evaluation = Purity (49:23), entropy = 0, highly certain of 1 class. (Weighted average purity and entropy 57:00)
- range of entropy depends on N classes [0,log_2 N).

Video 23:

- association rules = co-occurences
- support count = just the no. 
- support = percentage. (no/total transactions)
- frequent itemset, those itemsets who are equal or greater than min support.
- confidence = no/with less than total
- for customer id, merge his items which are bought.
- total no itemsets are 3^d-2^(d+1)+1, with 3 items out of d then = ^dC_3
- Frequent itemset generation strategies. Reduce all, M(no of candidates), N (no of transactions), Comparison (NM)
- For M:
  - Apriori Principle. If an itemset is frequent then all its subset sets would be frequent as well.
  - combinatorics, watch 31:00
  - keep generating k+1 candidate itemsets until no new frequent itemsets are found.
  - when combining 2 subset, then make sure k are common only k+1 is different. Watch 40:50
  - pruning, we should have all the k itemset in the original if not then prune it.
- For N, use hash-pruning algorithms. Watch 51:00
  
Video 24:

- Watch 8:00, how do we store the itemsets generation.
- how to select operations using hash tree. This way no of comparisons are reduced.
- factors which can affect complexity, different min support threshold, no of items, size of database.
- alternative search methods
   - general to specific (when less items)
   - specific to general (when more items)
   - breadth first
   - depth first
- bottlenecks, multiple scans in apriori
- Other way ECLAT for frequent itemset generation.
   - vertical data structure
- Another way for frequent itemset generation is FP tree (frequent pattern tree)
   - watch 44:40
   - find support for each item,
   - then discard infrequent items
   - sort frequent items in decreasing order based on support.
   - bottom up algorithm, a prefix path tree for E item here dont update the count
   - in the conditional fp-tree update the count and remove the item from fp tree.
- adv = fp tree is very fast.
- maximal frequent itemset = those sets whose none superset are frequent subset.
- closed itemset = if none of its immediate supersets has the same support as the itemset. min support should be there.
- Alll maximals are closed itemset and all closed are frequent itemsets.
- Rule generation
    - 2^k-2 association rules possible.
    - confidence is anti-monotone wrt number of items on the RHS of the rule.
    - check video 25, 52:30, 54:00
 
Video 25:

- Confidence can be misleading, Tea coffee example, watch 57:30. Drawback
- statistical independence, P(S^B)> P(S) x P(B) = positvely correlated.
- lift, interest = s(X,Y)/ (s(X)*s(Y)). If lift < 1 therefore negatively associated. It also has some drawback.






